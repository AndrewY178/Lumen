{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üßô Potion Flow Monitoring Dashboard - Data Processing\n",
        "\n",
        "## Hackathon Challenge: Poyo's Potion Factory Monitor\n",
        "\n",
        "This notebook processes data from the Potion Factory API to:\n",
        "- Load and analyze cauldron level time series data\n",
        "- Calculate fill rates for each cauldron\n",
        "- Detect drain events (potion collections)\n",
        "- Match transport tickets to drain events\n",
        "- Identify discrepancies and suspicious activity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'requests'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (15, 6)\n",
        "\n",
        "print(\"‚úì All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Processor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "class CauldronDataProcessor:\n",
        "    \"\"\"Main class for processing cauldron monitoring data.\"\"\"\n",
        "    \n",
        "    BASE_URL = \"https://hackutd2025.eog.systems\"\n",
        "    MARKET_UNLOAD_TIME_MINUTES = 15  # Time witches take to unload at market\n",
        "    \n",
        "    def __init__(self, cache_enabled: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the data processor.\n",
        "        \n",
        "        Args:\n",
        "            cache_enabled: Whether to cache API responses locally\n",
        "        \"\"\"\n",
        "        self.cache_enabled = cache_enabled\n",
        "        self.data_cache = {}\n",
        "        self.fill_rates_cache = {}  # Cache calculated fill rates\n",
        "        \n",
        "    def fetch_api_data(self, endpoint: str, use_cache: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Fetch data from API endpoint with optional caching.\n",
        "        \n",
        "        Args:\n",
        "            endpoint: API endpoint path\n",
        "            use_cache: Whether to use cached data if available\n",
        "            \n",
        "        Returns:\n",
        "            JSON response as dictionary\n",
        "        \"\"\"\n",
        "        cache_key = endpoint\n",
        "        \n",
        "        if use_cache and cache_key in self.data_cache:\n",
        "            print(f\"‚úì Using cached data for {endpoint}\")\n",
        "            return self.data_cache[cache_key]\n",
        "        \n",
        "        url = f\"{self.BASE_URL}{endpoint}\"\n",
        "        print(f\"‚¨á Fetching data from {endpoint}...\")\n",
        "        \n",
        "        try:\n",
        "            response = requests.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            \n",
        "            if self.cache_enabled:\n",
        "                self.data_cache[cache_key] = data\n",
        "                \n",
        "            print(f\"‚úì Successfully fetched {endpoint}\")\n",
        "            return data\n",
        "            \n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚úó Error fetching {endpoint}: {e}\")\n",
        "            return {}\n",
        "    \n",
        "    def fetch_travel_times(self, cauldron_ids: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Fetch travel times from each cauldron to the market.\n",
        "        \n",
        "        Args:\n",
        "            cauldron_ids: List of cauldron IDs to fetch travel times for\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary mapping cauldron_id to travel time in minutes\n",
        "        \"\"\"\n",
        "        print(\"\\nüîÑ Fetching travel times to market...\\n\")\n",
        "        travel_times = {}\n",
        "        \n",
        "        for cauldron_id in cauldron_ids:\n",
        "            neighbors = self.fetch_api_data(f'/api/Information/graph/neighbors/{cauldron_id}')\n",
        "            \n",
        "            if isinstance(neighbors, list):\n",
        "                for neighbor in neighbors:\n",
        "                    if neighbor.get('to', '').startswith('market'):\n",
        "                        # Parse time string format \"HH:MM:SS\" to minutes\n",
        "                        cost_str = neighbor.get('cost', '00:00:00')\n",
        "                        time_parts = cost_str.split(':')\n",
        "                        if len(time_parts) == 3:\n",
        "                            hours = int(time_parts[0])\n",
        "                            minutes = int(time_parts[1])\n",
        "                            seconds = int(time_parts[2])\n",
        "                            total_minutes = hours * 60 + minutes + seconds / 60\n",
        "                            travel_times[cauldron_id] = total_minutes\n",
        "                            print(f\"  {cauldron_id} ‚Üí market: {total_minutes:.1f} minutes\")\n",
        "                            break\n",
        "        \n",
        "        print(f\"\\n‚úì Fetched travel times for {len(travel_times)} cauldrons\\n\")\n",
        "        return travel_times\n",
        "    \n",
        "    def fetch_all_data(self) -> Dict[str, any]:\n",
        "        \"\"\"\n",
        "        Fetch all required data from API endpoints.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary containing all datasets\n",
        "        \"\"\"\n",
        "        print(\"\\nüîÑ Fetching all data from API...\\n\")\n",
        "        \n",
        "        datasets = {\n",
        "            'level_data': self.fetch_api_data('/api/Data/?start_date=0&end_date=2000000000'),\n",
        "            'tickets': self.fetch_api_data('/api/Tickets'),\n",
        "            'cauldrons': self.fetch_api_data('/api/Information/cauldrons'),\n",
        "            'network': self.fetch_api_data('/api/Information/network'),\n",
        "            'couriers': self.fetch_api_data('/api/Information/couriers'),\n",
        "            'market': self.fetch_api_data('/api/Information/market')\n",
        "        }\n",
        "        \n",
        "        print(\"\\n‚úì All data fetched successfully!\\n\")\n",
        "        return datasets\n",
        "    \n",
        "    def _normalize_json_data(self, data: Dict, key: Optional[str] = None) -> List:\n",
        "        \"\"\"Helper to normalize JSON data structure.\"\"\"\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "        elif key and key in data:\n",
        "            return data[key]\n",
        "        else:\n",
        "            return [data]\n",
        "    \n",
        "    def transform_level_data(self, level_data: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Transform raw level data into time-indexed DataFrame.\"\"\"\n",
        "        print(\"üîÑ Transforming potion level data...\")\n",
        "        records = self._normalize_json_data(level_data, 'data')\n",
        "        df = pd.json_normalize(records)\n",
        "        \n",
        "        if 'timestamp' in df.columns:\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "            df = df.set_index('timestamp').sort_index()\n",
        "        \n",
        "        print(f\"‚úì Transformed {len(df)} records ({df.index.min()} to {df.index.max()})\")\n",
        "        return df\n",
        "    \n",
        "    def transform_tickets(self, tickets_data: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Transform tickets data into DataFrame.\"\"\"\n",
        "        print(\"üîÑ Transforming ticket data...\")\n",
        "        tickets = self._normalize_json_data(tickets_data, 'transport_tickets')\n",
        "        df = pd.DataFrame(tickets)\n",
        "        \n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "        \n",
        "        print(f\"‚úì Transformed {len(df)} tickets | Total volume: {df['amount_collected'].sum():.2f}L\")\n",
        "        return df\n",
        "    \n",
        "    def transform_cauldrons(self, cauldrons_data: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Transform cauldrons metadata into DataFrame.\"\"\"\n",
        "        print(\"üîÑ Transforming cauldron metadata...\")\n",
        "        cauldrons = self._normalize_json_data(cauldrons_data)\n",
        "        df = pd.DataFrame(cauldrons)\n",
        "        \n",
        "        if 'id' in df.columns:\n",
        "            df = df.set_index('id')\n",
        "        \n",
        "        print(f\"‚úì Transformed {len(df)} cauldrons\")\n",
        "        return df\n",
        "\n",
        "print(\"‚úì CauldronDataProcessor class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fetch Data from API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Initialize processor\n",
        "processor = CauldronDataProcessor(cache_enabled=True)\n",
        "\n",
        "# Fetch all data\n",
        "raw_data = processor.fetch_all_data()\n",
        "\n",
        "# Transform into DataFrames\n",
        "df_levels = processor.transform_level_data(raw_data['level_data'])\n",
        "df_tickets = processor.transform_tickets(raw_data['tickets'])\n",
        "df_cauldrons = processor.transform_cauldrons(raw_data['cauldrons'])\n",
        "\n",
        "# Fetch travel times from cauldrons to market\n",
        "# Extract cauldron IDs from the columns\n",
        "cauldron_ids = [col.replace('cauldron_levels.', '') for col in df_levels.columns if 'cauldron' in col.lower()]\n",
        "travel_times = processor.fetch_travel_times(cauldron_ids)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Level measurements: {len(df_levels)} records\")\n",
        "print(f\"Transport tickets: {len(df_tickets)} tickets\")\n",
        "print(f\"Cauldrons tracked: {len(df_cauldrons)} cauldrons\")\n",
        "print(f\"Travel times fetched: {len(travel_times)} cauldrons\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Calculate Fill Rates\n",
        "\n",
        "Calculate the fill rate for each cauldron by analyzing periods where levels steadily increase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Quick data verification\n",
        "print(f\"‚úì Data loaded: {df_levels.shape[0]} records √ó {df_levels.shape[1]} cauldrons\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Let's examine one cauldron in detail to understand the patterns\n",
        "cauldron_cols = [col for col in df_levels.columns if 'cauldron' in col.lower()]\n",
        "\n",
        "for i in range(len(cauldron_cols)): #len(cauldron_cols)\n",
        "    # Pick first cauldron for detailed analysis\n",
        "    sample_col = cauldron_cols[i]\n",
        "    sample_data = df_levels[sample_col].dropna()\n",
        "    \n",
        "    print(f\"Analyzing {sample_col}...\")\n",
        "    print(f\"Data points: {len(sample_data)}\")\n",
        "    print(f\"Value range: {sample_data.min():.2f} to {sample_data.max():.2f}\")\n",
        "    \n",
        "    # Calculate rate of change (derivative)\n",
        "    time_diffs = sample_data.index.to_series().diff().dt.total_seconds() / 60  # minutes\n",
        "    level_diffs = sample_data.diff()\n",
        "    rates = level_diffs / time_diffs  # liters per minute\n",
        "    \n",
        "    print(f\"\\nRate of change statistics:\")\n",
        "    print(f\"  Mean rate: {rates.mean():.4f} L/min\")\n",
        "    print(f\"  Median rate: {rates.median():.4f} L/min\")\n",
        "    print(f\"  Std dev: {rates.std():.4f} L/min\")\n",
        "    print(f\"  Min rate: {rates.min():.4f} L/min\")\n",
        "    print(f\"  Max rate: {rates.max():.4f} L/min\")\n",
        "    \n",
        "    # Plot the data and its rate of change\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
        "    \n",
        "    # Plot levels\n",
        "    sample_data.plot(ax=ax1, linewidth=1)\n",
        "    ax1.set_title(f'{sample_col} - Level Over Time', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Level (liters)', fontsize=12)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot rate of change\n",
        "    rates.plot(ax=ax2, linewidth=1, color='orange')\n",
        "    ax2.axhline(y=rates.median(), color='green', linestyle='--', label=f'Median rate: {rates.median():.4f}')\n",
        "    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Zero rate')\n",
        "    ax2.set_title(f'{sample_col} - Rate of Change Over Time', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Rate (L/min)', fontsize=12)\n",
        "    ax2.set_xlabel('Time', fontsize=12)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Look for periods where rate drops significantly\n",
        "    print(\"\\nLooking for rate anomalies...\")\n",
        "    median_rate = rates.median()\n",
        "    std_rate = rates.std()\n",
        "    \n",
        "    # Define threshold as significant deviation from median\n",
        "    threshold = median_rate - 2 * std_rate\n",
        "    print(f\"Threshold for anomaly detection: {threshold:.4f} L/min\")\n",
        "    \n",
        "    anomalies = rates[rates < threshold]\n",
        "    print(f\"Found {len(anomalies)} time points with anomalous rates\")\n",
        "else:\n",
        "    print(\"No cauldron columns found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Detect Drain Events (Sharp Drop Detection)\n",
        "\n",
        "Drain events are when potion is collected from cauldrons. From the visualization in Section 4.5, we can see clear drain events characterized by:\n",
        "- **Sharp drops in cauldron levels** (sawtooth pattern)\n",
        "- **Sharp negative spikes in rate of change** (rate goes significantly negative)\n",
        "\n",
        "The algorithm detects these by identifying periods where the rate becomes significantly negative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def _calculate_drain_volume(level_before: float, level_after: float, \n",
        "                           fill_rate: float, drain_duration: float) -> float:\n",
        "    \"\"\"Calculate total potion collected during a drain event.\"\"\"\n",
        "    level_drop = level_before - level_after\n",
        "    potion_generated = fill_rate * drain_duration\n",
        "    return level_drop + potion_generated\n",
        "\n",
        "def _calculate_ticket_date(drain_end: datetime, travel_time_minutes: float) -> datetime.date:\n",
        "    \"\"\"Calculate the date when ticket would be reported (after travel to market).\"\"\"\n",
        "    arrival_at_market = drain_end + timedelta(minutes=travel_time_minutes)\n",
        "    return arrival_at_market.date()\n",
        "\n",
        "def detect_drain_events(series: pd.Series, cauldron_id: str, \n",
        "                                       normal_fill_rate: float,\n",
        "                                       travel_time_minutes: float = 0,\n",
        "                                       negative_rate_threshold: float = -0.05,\n",
        "                                       min_drop_volume: float = 20.0) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Detect drain events by identifying sharp negative rate spikes.\n",
        "    \n",
        "    Based on the visualization, drain events show as:\n",
        "    - Sharp drops in level (sawtooth pattern)\n",
        "    - Sharp negative spikes in rate of change\n",
        "    - Brief duration events\n",
        "    \n",
        "    Args:\n",
        "        series: Time series of cauldron levels\n",
        "        cauldron_id: ID of the cauldron\n",
        "        normal_fill_rate: Normal fill rate in units per minute\n",
        "        travel_time_minutes: Travel time from cauldron to market (for ticket date assignment)\n",
        "        negative_rate_threshold: Rate threshold to identify drains (e.g., -0.05 means rate < -0.05)\n",
        "        min_drop_volume: Minimum volume drop to consider as drain event (liters)\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with detected drain events\n",
        "    \"\"\"\n",
        "    series = series.dropna()\n",
        "    if len(series) < 3:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Calculate instantaneous rates\n",
        "    time_diffs = series.index.to_series().diff().dt.total_seconds() / 60  # minutes\n",
        "    level_diffs = series.diff()\n",
        "    rates = level_diffs / time_diffs  # liters per minute\n",
        "    \n",
        "    # Find points where rate goes significantly negative (sharp drops)\n",
        "    is_draining = rates < negative_rate_threshold\n",
        "    \n",
        "    # Find drain events by looking for consecutive negative rate periods\n",
        "    drain_events = []\n",
        "    in_drain = False\n",
        "    drain_start = None\n",
        "    drain_start_idx = None\n",
        "    peak_level = None\n",
        "    peak_idx = None\n",
        "    \n",
        "    for i in range(len(series)):\n",
        "        timestamp = series.index[i]\n",
        "        level = series.iloc[i]\n",
        "        is_drain_point = is_draining.iloc[i] if i < len(is_draining) else False\n",
        "        \n",
        "        if not in_drain:\n",
        "            # Track the peak level before drain\n",
        "            if peak_level is None or level > peak_level:\n",
        "                peak_level = level\n",
        "                peak_idx = i\n",
        "                peak_time = timestamp\n",
        "            \n",
        "            # Start of drain detected\n",
        "            if is_drain_point:\n",
        "                in_drain = True\n",
        "                drain_start = timestamp\n",
        "                drain_start_idx = i\n",
        "                \n",
        "        else:\n",
        "            # We're in a drain period\n",
        "            if not is_drain_point:\n",
        "                # End of drain - rate returned to positive/normal\n",
        "                drain_end = timestamp\n",
        "                drain_end_idx = i\n",
        "                level_after = level\n",
        "                \n",
        "                # Use the peak level before drain started\n",
        "                level_before = peak_level\n",
        "                level_drop = level_before - level_after\n",
        "                \n",
        "                # Calculate drain duration\n",
        "                drain_duration = (drain_end - drain_start).total_seconds() / 60\n",
        "                \n",
        "                # Calculate volume collected\n",
        "                # During drain, potion continues filling at normal rate\n",
        "                potion_generated_during_drain = normal_fill_rate * drain_duration\n",
        "                total_collected = level_drop + potion_generated_during_drain\n",
        "                \n",
        "                # Only record if significant drop occurred\n",
        "                if level_drop >= min_drop_volume and total_collected > 0:\n",
        "                    ticket_date = _calculate_ticket_date(drain_end, travel_time_minutes)\n",
        "                    \n",
        "                    drain_events.append({\n",
        "                        'cauldron_id': cauldron_id,\n",
        "                        'start_time': drain_start,\n",
        "                        'end_time': drain_end,\n",
        "                        'date': drain_end.date(),\n",
        "                        'ticket_date': ticket_date,\n",
        "                        'level_before': level_before,\n",
        "                        'level_after': level_after,\n",
        "                        'level_drop': level_drop,\n",
        "                        'drain_duration_min': drain_duration,\n",
        "                        'potion_generated_during_drain': potion_generated_during_drain,\n",
        "                        'total_collected': total_collected,\n",
        "                        'min_rate_during_drain': rates.iloc[drain_start_idx:drain_end_idx].min()\n",
        "                    })\n",
        "                \n",
        "                # Reset for next drain\n",
        "                in_drain = False\n",
        "                drain_start = None\n",
        "                drain_start_idx = None\n",
        "                peak_level = level\n",
        "                peak_idx = i\n",
        "    \n",
        "    return pd.DataFrame(drain_events)\n",
        "\n",
        "\n",
        "# Detect drain events for all cauldrons using sharp drop detection\n",
        "print(\"\\nüîÑ Detecting drain events by identifying sharp negative rate spikes...\\n\")\n",
        "print(\"(Looking for clear drain events with negative rates and level drops)\\n\")\n",
        "\n",
        "all_drain_events = []\n",
        "\n",
        "for col in cauldron_cols:\n",
        "    cauldron_id = col.replace('cauldron_levels.', '')\n",
        "    fill_rate = fill_rates.get(col, np.nan)\n",
        "    travel_time = travel_times.get(cauldron_id, 0)\n",
        "    \n",
        "    if np.isnan(fill_rate) or fill_rate <= 0:\n",
        "        print(f\"{cauldron_id}: Skipping (invalid fill rate)\")\n",
        "        continue\n",
        "    \n",
        "    # Detect sharp drain events with travel time adjustment\n",
        "    events = detect_drain_events(\n",
        "        df_levels[col], \n",
        "        cauldron_id, \n",
        "        fill_rate,\n",
        "        travel_time_minutes=travel_time,  # Adjust ticket date for travel time\n",
        "        negative_rate_threshold=-0.05,    # Rate must go below -0.05 L/min\n",
        "        min_drop_volume=20.0               # Level must drop at least 20 liters\n",
        "    )\n",
        "    \n",
        "    all_drain_events.append(events)\n",
        "    print(f\"{cauldron_id}: {len(events)} drain events detected\")\n",
        "\n",
        "# Combine all drain events\n",
        "if len(all_drain_events) > 0 and any(len(df) > 0 for df in all_drain_events):\n",
        "    df_drain_events = pd.concat([df for df in all_drain_events if len(df) > 0], ignore_index=True)\n",
        "    df_drain_events = df_drain_events.sort_values('end_time')\n",
        "    \n",
        "    print(f\"\\n‚úì Total drain events detected: {len(df_drain_events)}\")\n",
        "    print(f\"  Total potion collected: {df_drain_events['total_collected'].sum():.2f} liters\")\n",
        "else:\n",
        "    df_drain_events = pd.DataFrame()\n",
        "    print(\"\\n‚ö†Ô∏è No drain events detected with current parameters\")\n",
        "    print(\"Try adjusting negative_rate_threshold (more negative = more sensitive)\")\n",
        "    print(\"or min_drop_volume (lower = catch smaller drains)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Display sample drain events\n",
        "print(\"\\nüìä SAMPLE DRAIN EVENTS:\")\n",
        "if len(df_drain_events) > 0:\n",
        "    print(df_drain_events.head(15).to_string(index=False))\n",
        "else:\n",
        "    print(\"No drain events detected. See section 6.5 to adjust detection parameters.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Match Drain Events to Tickets\n",
        "\n",
        "Now we match each detected drain event to the corresponding transport ticket. \n",
        "\n",
        "**Important:** Ticket dates account for travel time to market. If a drain ends close to midnight and the courier needs time to reach the market, the ticket will be dated for the day the courier arrives (potentially the next day).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def match_drains_to_tickets(df_drains: pd.DataFrame, df_tickets: pd.DataFrame, \n",
        "                            tolerance_pct: float = 10.0) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Match each drain event to tickets reported on the same day.\n",
        "    \n",
        "    For each drain event, finds tickets on the same date and cauldron,\n",
        "    then determines if the drain was properly reported, under-reported, or not reported.\n",
        "    \n",
        "    Args:\n",
        "        df_drains: DataFrame with detected drain events\n",
        "        df_tickets: DataFrame with ticket data\n",
        "        tolerance_pct: Percentage tolerance for volume matching\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with each drain and its ticket match status\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for _, drain in df_drains.iterrows():\n",
        "        # Use ticket_date which accounts for travel time to market\n",
        "        # If drain ends close to midnight, courier arrives next day\n",
        "        ticket_date = drain.get('ticket_date', drain['date'])  # Fall back to drain date if no ticket_date\n",
        "        cauldron_id = drain['cauldron_id']\n",
        "        drain_amount = drain['total_collected']\n",
        "        \n",
        "        # Find all tickets for this cauldron on the ticket date (not drain date)\n",
        "        matching_tickets = df_tickets[\n",
        "            (df_tickets['cauldron_id'] == cauldron_id) & \n",
        "            (df_tickets['date'].dt.date == ticket_date)\n",
        "        ]\n",
        "        \n",
        "        if len(matching_tickets) == 0:\n",
        "            # No ticket found for this drain\n",
        "            results.append({\n",
        "                'drain_id': f\"{cauldron_id}_{drain['end_time']}\",\n",
        "                'cauldron_id': cauldron_id,\n",
        "                'drain_date': drain['date'],  # Actual drain date\n",
        "                'ticket_date': ticket_date,    # Expected ticket date (after travel)\n",
        "                'drain_time': drain['end_time'],\n",
        "                'drain_amount': drain_amount,\n",
        "                'ticket_id': None,\n",
        "                'ticket_amount': None,\n",
        "                'difference': None,\n",
        "                'difference_pct': None,\n",
        "                'status': 'NO_TICKET_FOUND',\n",
        "                'notes': 'Drain event detected but no ticket reported on expected date'\n",
        "            })\n",
        "        else:\n",
        "            # Find best matching ticket by volume\n",
        "            best_ticket = None\n",
        "            min_diff = float('inf')\n",
        "            \n",
        "            for _, ticket in matching_tickets.iterrows():\n",
        "                diff = abs(drain_amount - ticket['amount_collected'])\n",
        "                if diff < min_diff:\n",
        "                    min_diff = diff\n",
        "                    best_ticket = ticket\n",
        "            \n",
        "            ticket_amount = best_ticket['amount_collected']\n",
        "            \n",
        "            # Calculate percentage difference\n",
        "            pct_diff = (min_diff / drain_amount) * 100 if drain_amount > 0 else 0\n",
        "            \n",
        "            # Determine status\n",
        "            if abs(pct_diff) <= tolerance_pct:\n",
        "                status = 'MATCHED'\n",
        "                notes = f'Ticket matches drain (difference: {pct_diff:.2f}%)'\n",
        "            elif ticket_amount < drain_amount:\n",
        "                # Ticket reports less than what was drained\n",
        "                under_reported_amount = drain_amount - ticket_amount\n",
        "                status = 'UNDER_REPORTED'\n",
        "                notes = f'Ticket under-reports by {under_reported_amount:.2f}L ({pct_diff:.2f}%)'\n",
        "            else:\n",
        "                # Ticket reports more than what was drained\n",
        "                over_reported_amount = ticket_amount - drain_amount\n",
        "                status = 'OVER_REPORTED'\n",
        "                notes = f'Ticket over-reports by {over_reported_amount:.2f}L ({pct_diff:.2f}%)'\n",
        "            \n",
        "            results.append({\n",
        "                'drain_id': f\"{cauldron_id}_{drain['end_time']}\",\n",
        "                'cauldron_id': cauldron_id,\n",
        "                'drain_date': drain['date'],  # Actual drain date\n",
        "                'ticket_date': ticket_date,    # Expected ticket date (after travel)\n",
        "                'drain_time': drain['end_time'],\n",
        "                'drain_amount': drain_amount,\n",
        "                'ticket_id': best_ticket['ticket_id'],\n",
        "                'ticket_amount': ticket_amount,\n",
        "                'difference': ticket_amount - drain_amount,  # Positive = over-reported, Negative = under-reported\n",
        "                'difference_pct': pct_diff,\n",
        "                'status': status,\n",
        "                'notes': notes\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# Match drain events to tickets\n",
        "print(\"\\nüîÑ Matching drain events to tickets...\\n\")\n",
        "print(\"For each detected drain, finding tickets reported on the same day...\\n\")\n",
        "\n",
        "if len(df_drain_events) > 0:\n",
        "    df_matched = match_drains_to_tickets(df_drain_events, df_tickets, tolerance_pct=2.0)\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"=\"*60)\n",
        "    print(\"DRAIN EVENT MATCHING SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total drain events: {len(df_matched)}\")\n",
        "    print(f\"Matched drains: {len(df_matched[df_matched['status'] == 'MATCHED'])}\")\n",
        "    print(f\"Under-reported drains: {len(df_matched[df_matched['status'] == 'UNDER_REPORTED'])}\")\n",
        "    print(f\"Over-reported drains: {len(df_matched[df_matched['status'] == 'OVER_REPORTED'])}\")\n",
        "    print(f\"No ticket found: {len(df_matched[df_matched['status'] == 'NO_TICKET_FOUND'])}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Additional statistics\n",
        "    if len(df_matched[df_matched['status'] == 'UNDER_REPORTED']) > 0:\n",
        "        under_reported = df_matched[df_matched['status'] == 'UNDER_REPORTED']\n",
        "        total_under_reported = under_reported['difference'].sum() * -1  # Make positive\n",
        "        print(f\"\\n‚ö†Ô∏è Total under-reported volume: {total_under_reported:.2f} liters\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cannot match - no drain events detected yet.\")\n",
        "    df_matched = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4. Color-Coded Drain Event Visualization\n",
        "\n",
        "Visualize drains with color coding based on ticket matching status:\n",
        "- üü¢ Green = Matched\n",
        "- üü° Yellow = Under-reported  \n",
        "- üî¥ Red = No ticket found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Color-coded drain visualization based on ticket matching\n",
        "if len(df_drain_events) > 0 and len(df_matched) > 0:\n",
        "    # Create timeline comparison for each cauldron\n",
        "    fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    all_cauldron_ids = sorted(df_tickets['cauldron_id'].unique())\n",
        "    \n",
        "    # Define colors for each status\n",
        "    status_colors = {\n",
        "        'MATCHED': 'green',\n",
        "        'UNDER_REPORTED': 'gold',\n",
        "        'OVER_REPORTED': 'orange',\n",
        "        'NO_TICKET_FOUND': 'red'\n",
        "    }\n",
        "    \n",
        "    for idx, cauldron_id in enumerate(all_cauldron_ids):\n",
        "        if idx >= len(axes):\n",
        "            break\n",
        "            \n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Get tickets for this cauldron\n",
        "        cauldron_tickets = df_tickets[df_tickets['cauldron_id'] == cauldron_id].sort_values('date')\n",
        "        \n",
        "        # Get matched drain info for this cauldron\n",
        "        cauldron_matches = df_matched[df_matched['cauldron_id'] == cauldron_id].sort_values('drain_time')\n",
        "        \n",
        "        # Plot tickets as red circles\n",
        "        if len(cauldron_tickets) > 0:\n",
        "            ticket_dates = pd.to_datetime(cauldron_tickets['date'])\n",
        "            ticket_amounts = cauldron_tickets['amount_collected']\n",
        "            ax.scatter(ticket_dates, ticket_amounts, color='red', marker='o', \n",
        "                      s=100, alpha=0.7, label=f'Tickets ({len(cauldron_tickets)})', zorder=3)\n",
        "            \n",
        "            # Add ticket labels\n",
        "            for _, ticket in cauldron_tickets.iterrows():\n",
        "                ax.text(pd.to_datetime(ticket['date']), ticket['amount_collected'], \n",
        "                       f\"{ticket['amount_collected']:.0f}L\", \n",
        "                       fontsize=7, ha='right', va='bottom')\n",
        "        \n",
        "        # Plot drain events with color coding by status\n",
        "        if len(cauldron_matches) > 0:\n",
        "            for status, color in status_colors.items():\n",
        "                status_drains = cauldron_matches[cauldron_matches['status'] == status]\n",
        "                \n",
        "                if len(status_drains) > 0:\n",
        "                    drain_dates = pd.to_datetime(status_drains['drain_time'])\n",
        "                    drain_amounts = status_drains['drain_amount']\n",
        "                    \n",
        "                    ax.scatter(drain_dates, drain_amounts, color=color, marker='s', \n",
        "                              s=100, alpha=0.7, label=f'{status.replace(\"_\", \" \").title()} ({len(status_drains)})', \n",
        "                              zorder=2, edgecolors='black', linewidths=0.5)\n",
        "            \n",
        "            # Add drain labels\n",
        "            for _, match in cauldron_matches.iterrows():\n",
        "                color = status_colors.get(match['status'], 'blue')\n",
        "                ax.text(pd.to_datetime(match['drain_time']), match['drain_amount'], \n",
        "                       f\"{match['drain_amount']:.0f}L\", \n",
        "                       fontsize=7, ha='left', va='top', color=color)\n",
        "        \n",
        "        ax.set_title(f\"{cauldron_id}\", fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Date', fontsize=9)\n",
        "        ax.set_ylabel('Volume (liters)', fontsize=9)\n",
        "        ax.legend(fontsize=7, loc='best')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.tick_params(labelsize=8)\n",
        "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(len(all_cauldron_ids), len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Color-Coded Drain Events vs Tickets by Cauldron', \n",
        "                 fontsize=14, fontweight='bold', y=1.001)\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No drain events or matching data to visualize\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Show matched drains sample\n",
        "print(\"\\n‚úì SUCCESSFULLY MATCHED DRAINS (Sample):\")\n",
        "if len(df_matched) > 0:\n",
        "    matched = df_matched[df_matched['status'] == 'MATCHED']\n",
        "    if len(matched) > 0:\n",
        "        print(f\"Showing first 10 of {len(matched)} matched drains:\\n\")\n",
        "        print(matched[['cauldron_id', 'drain_date', 'drain_amount', 'ticket_amount', \n",
        "                      'difference_pct']].head(10).to_string(index=False))\n",
        "    else:\n",
        "        print(\"No matched drains found!\")\n",
        "else:\n",
        "    print(\"No drain events to analyze.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Visualize tickets and drains timeline\n",
        "if len(df_drain_events) > 0:\n",
        "    # Create timeline comparison for each cauldron\n",
        "    fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    all_cauldron_ids = sorted(df_tickets['cauldron_id'].unique())\n",
        "    \n",
        "    for idx, cauldron_id in enumerate(all_cauldron_ids):\n",
        "        if idx >= len(axes):\n",
        "            break\n",
        "            \n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Get tickets for this cauldron\n",
        "        cauldron_tickets = df_tickets[df_tickets['cauldron_id'] == cauldron_id].sort_values('date')\n",
        "        \n",
        "        # Get drain events for this cauldron\n",
        "        cauldron_drains = df_drain_events[df_drain_events['cauldron_id'] == cauldron_id].sort_values('end_time')\n",
        "        \n",
        "        # Plot tickets as red markers\n",
        "        if len(cauldron_tickets) > 0:\n",
        "            ticket_dates = pd.to_datetime(cauldron_tickets['date'])\n",
        "            ticket_amounts = cauldron_tickets['amount_collected']\n",
        "            ax.scatter(ticket_dates, ticket_amounts, color='red', marker='o', \n",
        "                      s=100, alpha=0.7, label=f'Tickets ({len(cauldron_tickets)})', zorder=3)\n",
        "            \n",
        "            # Add ticket labels\n",
        "            for _, ticket in cauldron_tickets.iterrows():\n",
        "                ax.text(pd.to_datetime(ticket['date']), ticket['amount_collected'], \n",
        "                       f\"{ticket['amount_collected']:.0f}L\", \n",
        "                       fontsize=7, ha='right', va='bottom')\n",
        "        \n",
        "        # Plot drain events as blue markers\n",
        "        if len(cauldron_drains) > 0:\n",
        "            drain_dates = pd.to_datetime(cauldron_drains['end_time'])\n",
        "            drain_amounts = cauldron_drains['total_collected']\n",
        "            ax.scatter(drain_dates, drain_amounts, color='blue', marker='s', \n",
        "                      s=100, alpha=0.7, label=f'Drains ({len(cauldron_drains)})', zorder=2)\n",
        "            \n",
        "            # Add drain labels\n",
        "            for _, drain in cauldron_drains.iterrows():\n",
        "                ax.text(pd.to_datetime(drain['end_time']), drain['total_collected'], \n",
        "                       f\"{drain['total_collected']:.0f}L\", \n",
        "                       fontsize=7, ha='left', va='top', color='blue')\n",
        "        \n",
        "        ax.set_title(f\"{cauldron_id}\", fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Date', fontsize=9)\n",
        "        ax.set_ylabel('Volume (liters)', fontsize=9)\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.tick_params(labelsize=8)\n",
        "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(len(all_cauldron_ids), len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Tickets (Red Circles) vs Drain Events (Blue Squares) by Cauldron', \n",
        "                 fontsize=14, fontweight='bold', y=1.001)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüìä Red circles = Tickets (from tickets API)\")\n",
        "    print(\"üìä Blue squares = Detected drain events (from level data)\")\n",
        "    print(\"üìä They should align closely if matching is working correctly\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No drain events to compare with tickets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Create a detailed comparison table\n",
        "if len(df_drain_events) > 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DETAILED TICKET vs DRAIN COMPARISON BY DATE\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for cauldron_id in sorted(df_tickets['cauldron_id'].unique()):\n",
        "        cauldron_tickets = df_tickets[df_tickets['cauldron_id'] == cauldron_id].sort_values('date')\n",
        "        cauldron_drains = df_drain_events[df_drain_events['cauldron_id'] == cauldron_id].sort_values('end_time')\n",
        "        \n",
        "        if len(cauldron_tickets) > 0 or len(cauldron_drains) > 0:\n",
        "            print(f\"\\n{'‚îÄ'*80}\")\n",
        "            print(f\"üîÆ {cauldron_id.upper()}\")\n",
        "            print(f\"{'‚îÄ'*80}\")\n",
        "            \n",
        "            # Group drains by date\n",
        "            if len(cauldron_drains) > 0:\n",
        "                cauldron_drains_copy = cauldron_drains.copy()\n",
        "                cauldron_drains_copy['date_only'] = pd.to_datetime(cauldron_drains_copy['end_time']).dt.date\n",
        "                drains_by_date = cauldron_drains_copy.groupby('date_only')['total_collected'].agg(['count', 'sum'])\n",
        "            else:\n",
        "                drains_by_date = pd.DataFrame()\n",
        "            \n",
        "            # Create date range\n",
        "            all_dates = set()\n",
        "            if len(cauldron_tickets) > 0:\n",
        "                all_dates.update(cauldron_tickets['date'].dt.date)\n",
        "            if len(cauldron_drains) > 0:\n",
        "                all_dates.update(pd.to_datetime(cauldron_drains['end_time']).dt.date)\n",
        "            \n",
        "            if len(all_dates) > 0:\n",
        "                print(f\"{'Date':<12} | {'Tickets':<8} | {'Ticket Vol':<11} | {'Drains':<7} | {'Drain Vol':<11} | {'Match?':<6}\")\n",
        "                print(f\"{'-'*12}-+-{'-'*8}-+-{'-'*11}-+-{'-'*7}-+-{'-'*11}-+-{'-'*6}\")\n",
        "                \n",
        "                for date in sorted(all_dates):\n",
        "                    # Tickets on this date\n",
        "                    date_tickets = cauldron_tickets[cauldron_tickets['date'].dt.date == date]\n",
        "                    ticket_count = len(date_tickets)\n",
        "                    ticket_vol = date_tickets['amount_collected'].sum() if ticket_count > 0 else 0\n",
        "                    \n",
        "                    # Drains on this date\n",
        "                    if date in drains_by_date.index:\n",
        "                        drain_count = int(drains_by_date.loc[date, 'count'])\n",
        "                        drain_vol = drains_by_date.loc[date, 'sum']\n",
        "                    else:\n",
        "                        drain_count = 0\n",
        "                        drain_vol = 0\n",
        "                    \n",
        "                    # Check if they match\n",
        "                    match_status = \"‚úì\" if (ticket_count > 0 and drain_count > 0 and \n",
        "                                          abs(ticket_vol - drain_vol) < ticket_vol * 0.1) else \"‚úó\"\n",
        "                    \n",
        "                    if ticket_count > 0 or drain_count > 0:\n",
        "                        print(f\"{str(date):<12} | {ticket_count:>8} | {ticket_vol:>9.1f} L | \"\n",
        "                              f\"{drain_count:>7} | {drain_vol:>9.1f} L | {match_status:>6}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No drain events detected - cannot compare with tickets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Advanced Analytics & Forecasting\n",
        "\n",
        "Calculate overflow risk, capacity utilization, and system-wide metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Calculate overflow forecasting for each cauldron\n",
        "print(\"=\"*70)\n",
        "print(\"OVERFLOW RISK ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "overflow_analysis = []\n",
        "\n",
        "for _, cauldron in df_cauldrons.iterrows():\n",
        "    cauldron_id = cauldron.name if isinstance(cauldron.name, str) else cauldron.get('id', '')\n",
        "    max_volume = cauldron.get('max_volume', np.nan)\n",
        "    \n",
        "    # Get current level (last recorded)\n",
        "    cauldron_col = f'cauldron_levels.{cauldron_id}'\n",
        "    if cauldron_col in df_levels.columns:\n",
        "        current_level = df_levels[cauldron_col].iloc[-1]\n",
        "        \n",
        "        # Get fill rate\n",
        "        fill_rate_data = df_fill_rates[df_fill_rates['cauldron'] == cauldron_col]\n",
        "        if len(fill_rate_data) > 0:\n",
        "            fill_rate_per_min = fill_rate_data['fill_rate_per_min'].values[0]\n",
        "            \n",
        "            if not np.isnan(max_volume) and not np.isnan(fill_rate_per_min) and fill_rate_per_min > 0:\n",
        "                # Calculate time to overflow\n",
        "                remaining_capacity = max_volume - current_level\n",
        "                hours_to_overflow = (remaining_capacity / fill_rate_per_min) / 60\n",
        "                \n",
        "                # Capacity utilization\n",
        "                utilization_pct = (current_level / max_volume) * 100\n",
        "                \n",
        "                overflow_analysis.append({\n",
        "                    'cauldron_id': cauldron_id,\n",
        "                    'current_level': current_level,\n",
        "                    'max_volume': max_volume,\n",
        "                    'remaining_capacity': remaining_capacity,\n",
        "                    'utilization_pct': utilization_pct,\n",
        "                    'fill_rate_per_hour': fill_rate_per_min * 60,\n",
        "                    'hours_to_overflow': hours_to_overflow,\n",
        "                    'risk_level': 'HIGH' if hours_to_overflow < 12 else ('MEDIUM' if hours_to_overflow < 24 else 'LOW')\n",
        "                })\n",
        "\n",
        "df_overflow = pd.DataFrame(overflow_analysis)\n",
        "\n",
        "if len(df_overflow) > 0:\n",
        "    print(f\"\\n{'Cauldron':<15} {'Current':<10} {'Capacity':<10} {'Utilization':<12} {'Hours to':<12} {'Risk':<8}\")\n",
        "    print(f\"{'ID':<15} {'Level (L)':<10} {'(L)':<10} {'(%)':<12} {'Overflow':<12} {'Level':<8}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    for _, row in df_overflow.sort_values('hours_to_overflow').iterrows():\n",
        "        print(f\"{row['cauldron_id']:<15} {row['current_level']:>9.1f} {row['max_volume']:>9.0f} \"\n",
        "              f\"{row['utilization_pct']:>10.1f}% {row['hours_to_overflow']:>10.1f}h  {row['risk_level']:<8}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"‚ö†Ô∏è  {len(df_overflow[df_overflow['risk_level'] == 'HIGH'])} cauldrons at HIGH risk (< 12 hours to overflow)\")\n",
        "    print(f\"‚ö†Ô∏è  {len(df_overflow[df_overflow['risk_level'] == 'MEDIUM'])} cauldrons at MEDIUM risk (12-24 hours)\")\n",
        "    print(f\"‚úì  {len(df_overflow[df_overflow['risk_level'] == 'LOW'])} cauldrons at LOW risk (> 24 hours)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Unable to calculate overflow risk\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# System-wide discrepancy analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SYSTEM-WIDE DISCREPANCY SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(df_matched) > 0:\n",
        "    # Total volumes\n",
        "    total_drained = df_matched['drain_amount'].sum()\n",
        "    total_ticketed = df_matched[df_matched['ticket_amount'].notna()]['ticket_amount'].sum()\n",
        "    total_unaccounted = total_drained - total_ticketed\n",
        "    \n",
        "    # Breakdown by status\n",
        "    matched = df_matched[df_matched['status'] == 'MATCHED']\n",
        "    under_reported = df_matched[df_matched['status'] == 'UNDER_REPORTED']\n",
        "    over_reported = df_matched[df_matched['status'] == 'OVER_REPORTED']\n",
        "    no_ticket = df_matched[df_matched['status'] == 'NO_TICKET_FOUND']\n",
        "    \n",
        "    print(f\"\\nüìä VOLUME SUMMARY:\")\n",
        "    print(f\"  Total potion drained (detected):  {total_drained:>10.2f} L\")\n",
        "    print(f\"  Total potion ticketed (reported):  {total_ticketed:>10.2f} L\")\n",
        "    print(f\"  Total unaccounted:                 {total_unaccounted:>10.2f} L ({(total_unaccounted/total_drained)*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nüé´ MATCHING BREAKDOWN:\")\n",
        "    print(f\"  Matched drains:       {len(matched):>4} events | {matched['drain_amount'].sum():>8.2f} L\")\n",
        "    print(f\"  Under-reported:       {len(under_reported):>4} events | {abs(under_reported['difference'].sum()):>8.2f} L missing\")\n",
        "    if len(over_reported) > 0:\n",
        "        print(f\"  Over-reported:        {len(over_reported):>4} events | {over_reported['difference'].sum():>8.2f} L excess\")\n",
        "    print(f\"  No ticket found:      {len(no_ticket):>4} events | {no_ticket['drain_amount'].sum():>8.2f} L unlogged\")\n",
        "    \n",
        "    # Accuracy metric\n",
        "    accuracy = (len(matched) / len(df_matched)) * 100\n",
        "    print(f\"\\n‚úì Ticketing accuracy: {accuracy:.1f}% of drains properly reported\")\n",
        "    \n",
        "    # Top suspicious cauldrons\n",
        "    if len(under_reported) > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è  TOP UNDER-REPORTING CAULDRONS:\")\n",
        "        under_by_cauldron = under_reported.groupby('cauldron_id').agg({\n",
        "            'difference': lambda x: abs(x.sum()),\n",
        "            'drain_amount': 'count'\n",
        "        }).sort_values('difference', ascending=False).head(5)\n",
        "        \n",
        "        for cauldron_id, row in under_by_cauldron.iterrows():\n",
        "            print(f\"  {cauldron_id}: {row['difference']:.2f} L missing across {row['drain_amount']} events\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Executive Summary\n",
        "\n",
        "Final report with all key findings and recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" \"*25 + \"üßô POTION FACTORY MONITORING REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìÖ MONITORING PERIOD:\")\n",
        "print(f\"  Start: {df_levels.index.min()}\")\n",
        "print(f\"  End:   {df_levels.index.max()}\")\n",
        "print(f\"  Duration: {(df_levels.index.max() - df_levels.index.min()).days} days\")\n",
        "\n",
        "print(f\"\\nüè≠ FACILITY OVERVIEW:\")\n",
        "print(f\"  Active cauldrons: {len(cauldron_cols)}\")\n",
        "print(f\"  Data points collected: {len(df_levels):,}\")\n",
        "print(f\"  Average fill rate: {df_fill_rates['fill_rate_per_hour'].mean():.2f} L/hour\")\n",
        "\n",
        "if len(df_drain_events) > 0:\n",
        "    print(f\"\\nüíß COLLECTION ACTIVITY:\")\n",
        "    print(f\"  Total drain events detected: {len(df_drain_events)}\")\n",
        "    print(f\"  Total potion collected: {df_drain_events['total_collected'].sum():.2f} L\")\n",
        "    print(f\"  Average per collection: {df_drain_events['total_collected'].mean():.2f} L\")\n",
        "    print(f\"  Collections per cauldron: {len(df_drain_events)/len(cauldron_cols):.1f}\")\n",
        "\n",
        "if len(df_matched) > 0:\n",
        "    matched_pct = (len(df_matched[df_matched['status'] == 'MATCHED']) / len(df_matched)) * 100\n",
        "    print(f\"\\nüé´ TICKET VERIFICATION:\")\n",
        "    print(f\"  Total transport tickets: {len(df_tickets)}\")\n",
        "    print(f\"  Matching accuracy: {matched_pct:.1f}%\")\n",
        "    print(f\"  Suspicious discrepancies: {len(df_matched[df_matched['status'].isin(['UNDER_REPORTED', 'NO_TICKET_FOUND'])])}\")\n",
        "    \n",
        "    total_missing = abs(df_matched[df_matched['difference'] < 0]['difference'].sum())\n",
        "    if total_missing > 0:\n",
        "        print(f\"  ‚ö†Ô∏è  CRITICAL: {total_missing:.2f} L of potion unaccounted for\")\n",
        "\n",
        "if len(df_overflow) > 0:\n",
        "    high_risk = df_overflow[df_overflow['risk_level'] == 'HIGH']\n",
        "    print(f\"\\n‚ö° OVERFLOW RISK STATUS:\")\n",
        "    if len(high_risk) > 0:\n",
        "        print(f\"  ‚ö†Ô∏è  HIGH PRIORITY: {len(high_risk)} cauldrons need immediate attention\")\n",
        "        print(f\"  Cauldrons at risk: {', '.join(high_risk['cauldron_id'].values)}\")\n",
        "    else:\n",
        "        print(f\"  ‚úì  No immediate overflow risk detected\")\n",
        "    print(f\"  Average capacity utilization: {df_overflow['utilization_pct'].mean():.1f}%\")\n",
        "\n",
        "print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
        "if len(df_matched) > 0:\n",
        "    under_reported = df_matched[df_matched['status'] == 'UNDER_REPORTED']\n",
        "    no_ticket = df_matched[df_matched['status'] == 'NO_TICKET_FOUND']\n",
        "    \n",
        "    if len(under_reported) > 0:\n",
        "        print(f\"  ‚Ä¢ Investigate {len(under_reported)} under-reported collections\")\n",
        "    if len(no_ticket) > 0:\n",
        "        print(f\"  ‚Ä¢ Track down {len(no_ticket)} unlogged drain events\")\n",
        "\n",
        "if len(df_overflow) > 0:\n",
        "    high_risk = df_overflow[df_overflow['risk_level'] == 'HIGH']\n",
        "    if len(high_risk) > 0:\n",
        "        print(f\"  ‚Ä¢ Schedule immediate collections for high-risk cauldrons\")\n",
        "    \n",
        "    avg_hours = df_overflow['hours_to_overflow'].mean()\n",
        "    if avg_hours < 24:\n",
        "        print(f\"  ‚Ä¢ Increase courier frequency (avg {avg_hours:.1f}h to overflow)\")\n",
        "\n",
        "print(f\"  ‚Ä¢ Continue monitoring for anomalies\")\n",
        "print(f\"  ‚Ä¢ Verify travel time adjustments for accurate ticket matching\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" \"*30 + \"End of Report\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Plot fill rates comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "df_fill_rates_sorted = df_fill_rates.sort_values('fill_rate_per_hour')\n",
        "plt.barh(df_fill_rates_sorted['cauldron'], df_fill_rates_sorted['fill_rate_per_hour'])\n",
        "plt.xlabel('Fill Rate (liters/hour)', fontsize=12)\n",
        "plt.ylabel('Cauldron', fontsize=12)\n",
        "plt.title('Fill Rates by Cauldron', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Plot ticket status distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Status counts\n",
        "status_counts = df_matched['status'].value_counts()\n",
        "colors = {'MATCHED': 'green', 'SUSPICIOUS': 'orange', 'NO_DRAIN_FOUND': 'red'}\n",
        "status_colors = [colors.get(status, 'gray') for status in status_counts.index]\n",
        "\n",
        "ax1.bar(status_counts.index, status_counts.values, color=status_colors)\n",
        "ax1.set_ylabel('Count', fontsize=12)\n",
        "ax1.set_title('Ticket Status Distribution', fontsize=14, fontweight='bold')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Pie chart\n",
        "ax2.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%',\n",
        "        colors=status_colors, startangle=90)\n",
        "ax2.set_title('Ticket Status Proportion', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Export analysis results to CSV files\n",
        "print(\"üíæ Exporting analysis results...\\n\")\n",
        "\n",
        "df_drain_events.to_csv('drain_events.csv', index=False)\n",
        "df_matched.to_csv('ticket_matching.csv', index=False)\n",
        "df_fill_rates.to_csv('fill_rates.csv', index=False)\n",
        "\n",
        "print(\"‚úì Exported 3 files: drain_events.csv, ticket_matching.csv, fill_rates.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
